{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23c48a0",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "\n",
    "*El descenso de gradiente requería conocimiento de, bueno, el gradiente de su función de costo (MSE). Matemáticamente necesitamos las primeras derivadas parciales de todas las entradas. Esto es difícil e ineficiente si solo arrojas cálculo al problema. ¡Autodiff en modo inverso al rescate!. Optimizado para muchas entradas + salidas ferw (como una neurona). Calcula todas las derivadas parciales en # de salidas + 1 recorrido gráfico. Sigue siendo fundamentalmente un truco de cálculo, es complicado pero funciona. Esto es lo que usa tensorflow*\n",
    "\n",
    "\n",
    "# Softmax\n",
    "\n",
    "*Utilizado para la clasificación. Dada una puntuación para cada clase. Produce una probabilidad de cada clase. La clase con la mayor probabilidad es la \"respuesta\" que obtienes.*\n",
    "\n",
    "*En resumen. El descenso de gradiente es un algoritmo para minimizar el error en varios pasos. Autodiff es un truco de cálculo para encontrar los gradientes en el descenso de gradiente. Softmax es una función para elegir la clasificación más probable dados varios valores de entrada*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6f95b",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "**La inspiración biológica**\n",
    "\n",
    "*Las neuronas en su corteza cerebral están conectadas a través de axones. Una neurona \"dispara\" a la neurona que está conectada, cuando se activan suficientes señales de entrada. Muy simple a nivel de neurona individual, pero las capas de neuronas conectadas de esta manera pueden producir un comportamiento de aprendizaje. Miles de millones de neuronas, cada una con miles de conexiones, producen una mente*\n",
    "\n",
    "**Columnas corticales.**\n",
    "\n",
    "*Las neuronas en su corteza parecen estar dispuestas en muchas pilas, o \"columnas\" que procesan la información en paralelo. Las \"minicolumnas\" de alrededor de 100 neuronas están organizadas en \"hipercolumnas\" más grandes. Hay 100 millones de mini columnas en tu corteza. Esto es casualmente similar a cómo funcionan las GPU.*\n",
    "\n",
    "**La primera neurona artificial**\n",
    "\n",
    "*Una neurona artificial \"dispara\" si hay más de N conexiones de entrada activas. Dependiendo del número de conexiones de cada neurona de entrada, y si una conexión activa o suprime una neurona, puede construir construcciones lógicas AND, OR y NOT de esta manera. Este ejemplo implementaría C= A O B si el umbral en 2 entradas está activo.*\n",
    "\n",
    "**La unidad de umbral lineal (LTU).**\n",
    "\n",
    "*Agrega pesos a la entrada; La salida viene dada por una función de paso.  Ejemplo existen entrada 1 y entrada 2, la suma de los productos de entradas y sus pesos, salida 1 si suma >=0 ...*\n",
    "\n",
    "**Perceptron**\n",
    "\n",
    "*Una capa de LTU. Un perceptrón puede aprender reforzando pesas que conducen a un comportamiento correcto durante el entrenamiento. Esto también tiene una base biológica (\"célula que se dispara junta, se conecta entre sí\")*\n",
    "\n",
    "**Perceptrón multicapas.**\n",
    "\n",
    "*Adición de capas ocultas. Esta es una red neuronal profunda. Entrenarlos es más complicado, pero hablaremos de eso.*\n",
    "\n",
    "**Una red neuronal profunda en la actualidad**\n",
    "\n",
    "*Reemplace la función de activación por pasos con algo mejor. Aplique softmax a la salida. Entrenamiento utilizado descenso de gradiente*\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "*¿Cómo se entrenan las pesas de un MLP? ¿Cómo aprende?. Retropropagación... o más específicamente: ¡Descenso de gradiente usando autodiff de modo inverso! Para cada paso de entrenamiento: Calcule el error de salida. Calcula cuánto contribuyó cada neurona en la capa oculta anterior. Propague ese error en una pasada inversa. Ajustar los pesos para reducir el error mediante el descenso de gradiente*\n",
    "\n",
    "**Funciones de Activacion (aka rectifier)**\n",
    "\n",
    "*Las funciones de paso no funcionan con descenso de degradado. ¡No hay gradiente! Matemáticamente, no tienen ningún derivado útil. Alternativas: Función logística, Función tangente hiperbólica, Unidad lineal exponecial (ELU), Función ReLU (Unidad lineal rectificada), ReLU es común, rápido de calcular y funciona bien. También: \"Leaky ReLU\", \"Noisy ReLU\", ELU a veces puede conducir a un aprendizaje más rápido aunque*\n",
    "\n",
    "\n",
    "**Funciones de Optimizacion**\n",
    "\n",
    "*Hay optimizadores más rápidos (como en el aprendizaje más rápido) que el descenso de gradiente. Optimización del impulso. Introduce un término de impulso al descenso, por lo que se ralentiza a medida que las cosas comienzan a aplanarse y se acelera a medida que la pendiente es empinada. Gradiente acelerado de Nesterov. Un pequeño ajuste en la optimización del momento: calcula el impulso en función del gradiente ligeramente por delante de usted, no donde se encuentra. RMSProp. Tasa de aprendizaje adaptativo para ayudar a apuntar hacia el mínimo. Adán. Estimación adaptativa del momento-momento + RMSProp combinado, opción popular hoy en día, fácil de usar*\n",
    "\n",
    "**Evitar el sobreajuste.** \n",
    "\n",
    "*Con miles de pesos para ajustar, el sobreajuste es un problema. Parada temprana (cuando el rendimiento comienza a disminuir). Los términos de regularización se agregaron a la función de costos durante la capacitación. Dropout-ignore digamos el 50% de todas las neuronas al azar en cada paso de entrenamiento. ¡Funciona sorprendentemente bien! Forzar a su modelo a extender su aprendizaje*\n",
    "\n",
    "**Ajuste de la topología.**\n",
    "\n",
    "*El ensayo y error es una forma. Evalúe una red más pequeña con menos neuronas en las capas ocultas. Evalúe una red más grande con más capas. Intenta reducir el tamaño de cada capa a medida que avanzas, forma un embudo. Más capas pueden producir un aprendizaje más rápido. O simplemente usa más capas y neuronas de las que necesitas, y no te importa porque usas la parada temprana, usa \"zoológicos modelo\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674ba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
