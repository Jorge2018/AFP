{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7221cb65",
   "metadata": {},
   "source": [
    "# RNN's, para que sirven?\n",
    "\n",
    "*Datos de series temporales. Cuando desea predecir el comportamiento futuro basado en el comportamiento pasado. Registros web, registros de sensores, operaciones bursátiles. Dónde conducir su casr autónomo basado en trayectorias pasadas.  Datos que consisten en secuencias de longitud arbitraria. Traducción automática. Leyendas de las imágenes. Música generada por máquina*\n",
    "\n",
    "**Topologias**\n",
    "\n",
    "*Sequence to sequence, es decir, predecir los precios de las acciones en función de series de datos históricos. Sequence to vector, es decir, palabras en una oración a sentimiento. Vector to sequence, es decir, crear leyendas a partir de una imagen. Coder -> Decoder, Sequence -> vector-> sequence . es decir, traducción automática*\n",
    "\n",
    "\n",
    "**Entrenando RNN,s**\n",
    "\n",
    "*Retropropagación a través del tiempo. al igual que la propagación hacia atrás en MLP, pero aplicada a cada paso de tiempo. Todos esos pasos de tiempo se suman rápidamente. Termina pareciendo una red neuronal real muy, muy profunda. Puede limitar la propagación inversa a un número limitado de pasos de tiempo (retropropagación truncada a través del tiempo)*\n",
    "\n",
    "*Los pasos de tiempo anteriores se diluyen con el tiempo. Esto puede ser un problema, por ejemplo, al aprender estructuras de oraciones. Célula LSTM. Célula de memoria a corto plazo. Mantiene estados separados a corto y largo plazo. Célula GRU. Unidad recurrente cerrada. Célula LSTM simplificada que funciona igual de bien*\n",
    "\n",
    "*Es realmente difícil. Muy sensible a topologías, elección de hiperparámetros. Muy intensivo en recursos. Una elección equivocada puede conducir a una RNN que no converge en absoluto*\n",
    "\n",
    "# Ejercicio\n",
    "\n",
    "*Análisis de sentimiento de las críticas de películas*\n",
    "\n",
    "*Este cuaderno está inspirado en el imdb_lstm.py ejemplo que se envía con Keras.*\n",
    "\n",
    "*En realidad, es un gran ejemplo del uso de RNN. El conjunto de datos que estamos utilizando consiste en reseñas de películas generadas por el usuario y la clasificación de si al usuario le gustó la película o no en función de su calificación asociada.*\n",
    "\n",
    "*¡Así que vamos a usar un RNN para hacer análisis de sentimiento en reseñas de películas de texto completo!*\n",
    "\n",
    "*Piensa en lo increíble que es esto. Vamos a entrenar una red neuronal artificial para \"leer\" reseñas de películas y adivinar si al autor le gustó la película o no.*\n",
    "\n",
    "*Dado que comprender el lenguaje escrito requiere realizar un seguimiento de todas las palabras en una oración, necesitamos una red neuronal recurrente para mantener una \"memoria\" de las palabras que han venido antes a medida que \"lee\" oraciones a lo largo del tiempo.*\n",
    "\n",
    "*En particular, usaremos celdas LSTM (memoria larga a corto plazo) porque realmente no queremos \"olvidar\" palabras demasiado rápido: las palabras al principio de una oración pueden afectar significativamente el significado de esa oración.*\n",
    "\n",
    "*Comencemos importando las cosas que necesitamos:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf843009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97c512",
   "metadata": {},
   "source": [
    "*Ahora importamos nuestros datos de entrenamiento y pruebas. Especificamos que solo nos preocupamos por las 20,000 palabras más populares en el conjunto de datos para mantener las cosas algo manejables. El conjunto de datos incluye 5,000 revisiones de capacitación y 25,000 revisiones de pruebas por alguna razón.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a7a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "print('Cargando data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840d138",
   "metadata": {},
   "source": [
    "*Vamos a tener una idea de cómo se ven estos datos. Veamos la primera función de entrenamiento, que debería representar una reseña escrita de la película:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c19714f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd27ca4",
   "metadata": {},
   "source": [
    "*¡Eso no parece una crítica de película! Pero este conjunto de datos le ha ahorrado muchos problemas: ya han convertido palabras en índices basados en enteros. Las letras reales que componen una palabra realmente no importan en lo que respecta a nuestro modelo, lo que importa son las palabras mismas, y nuestro modelo necesita números para trabajar, no letras.*\n",
    "\n",
    "*Así que ten en cuenta que cada número en las características de entrenamiento representa alguna palabra específica. Sin embargo, es un fastidio que no podamos simplemente leer las reseñas en inglés como una verificación visceral para ver si el análisis de sentimientos realmente está funcionando.*\n",
    "\n",
    "*¿Qué aspecto tienen las etiquetas?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5ce9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff219961",
   "metadata": {},
   "source": [
    "*Son solo 0 o 1, lo que indica si el crítico dijo que le gustó la película o no.*\n",
    "\n",
    "*Entonces, para recapitular, tenemos un montón de reseñas de películas que se han convertido en vectores de palabras representadas por enteros, y una clasificación de sentimiento binario de la que aprender.*\n",
    "\n",
    "*Las RNN pueden explotar rápidamente, así que de nuevo para mantener las cosas manejables en nuestra pequeña PC, limitemos las revisiones a sus primeras 80 palabras:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cc5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb19a7",
   "metadata": {},
   "source": [
    "*¡Ahora configuremos nuestro modelo de red neuronal! Teniendo en cuenta lo complicada que es una red neuronal recurrente LSTM bajo el capó, es realmente sorprendente lo fácil que es hacerlo con Keras.*\n",
    "\n",
    "*Comenzaremos con una capa de incrustación: este es solo un paso que convierte los datos de entrada en vectores densos de tamaño fijo que se adaptan mejor a una red neuronal. Por lo general, esto se ve junto con datos de texto basados en índices como los que tenemos aquí. El 20,000 indica el tamaño del vocabulario (recuerde que dijimos que solo queríamos las 20,000 palabras principales) y 128 es la dimensión de salida de 128 unidades.*\n",
    "\n",
    "*A continuación, solo tenemos que configurar una capa LSTM para el propio RNN. Así de fácil. Especificamos 128 para que coincida con el tamaño de salida de la capa de incrustación, y los términos de abandono para evitar el sobreajuste, a lo que los RNN son particularmente propensos.*\n",
    "\n",
    "*Finalmente, solo necesitamos reducirlo a una sola neurona con una función de activación sigmoide para elegir nuestra clasificación de sentimiento binay de 0 o 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9e6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0dff0",
   "metadata": {},
   "source": [
    "*Como este es un problema de clasificación binaria, usaremos la función de pérdida de binary_crossentropy. Y el optimizador Adam suele ser una buena opción (siéntase libre de probar otros).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb71b49",
   "metadata": {},
   "source": [
    "*Ahora realmente entrenaremos nuestro modelo. Las RNN, como las de CNN, son muy pesadas en recursos. Mantener el tamaño del lote relativamente pequeño es la clave para permitir que esto se ejecute en su PC. En la palabra real, por supuesto, estaría aprovechando las GPU instaladas en muchas computadoras en un clúster para mejorar mucho esta escala*.\n",
    "\n",
    "## Advertencia\n",
    "\n",
    "*Esto tardará mucho tiempo en ejecutarse, ¡incluso en una PC rápida! No ejecute los siguientes bloques a menos que esté preparado para atar su computadora durante una hora o más.*\n",
    "\n",
    "*Ahora comencemos el entrenamiento. ¡Incluso con una GPU, esto llevará mucho tiempo!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a1c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 - 139s - loss: 0.4367 - accuracy: 0.7934 - val_loss: 0.3579 - val_accuracy: 0.8380 - 139s/epoch - 178ms/step\n",
      "Epoch 2/15\n",
      "782/782 - 131s - loss: 0.2560 - accuracy: 0.9004 - val_loss: 0.3808 - val_accuracy: 0.8384 - 131s/epoch - 168ms/step\n",
      "Epoch 3/15\n",
      "782/782 - 131s - loss: 0.1657 - accuracy: 0.9366 - val_loss: 0.4690 - val_accuracy: 0.8276 - 131s/epoch - 167ms/step\n",
      "Epoch 4/15\n",
      "782/782 - 130s - loss: 0.1103 - accuracy: 0.9598 - val_loss: 0.5585 - val_accuracy: 0.8247 - 130s/epoch - 166ms/step\n",
      "Epoch 5/15\n",
      "782/782 - 129s - loss: 0.0686 - accuracy: 0.9778 - val_loss: 0.7360 - val_accuracy: 0.8194 - 129s/epoch - 165ms/step\n",
      "Epoch 6/15\n",
      "782/782 - 128s - loss: 0.0515 - accuracy: 0.9824 - val_loss: 0.6593 - val_accuracy: 0.8202 - 128s/epoch - 164ms/step\n",
      "Epoch 7/15\n",
      "782/782 - 128s - loss: 0.0374 - accuracy: 0.9872 - val_loss: 0.8509 - val_accuracy: 0.8124 - 128s/epoch - 164ms/step\n",
      "Epoch 8/15\n",
      "782/782 - 157s - loss: 0.0275 - accuracy: 0.9919 - val_loss: 0.7934 - val_accuracy: 0.8172 - 157s/epoch - 200ms/step\n",
      "Epoch 9/15\n",
      "782/782 - 127s - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.8020 - val_accuracy: 0.8204 - 127s/epoch - 162ms/step\n",
      "Epoch 10/15\n",
      "782/782 - 127s - loss: 0.0233 - accuracy: 0.9928 - val_loss: 0.9175 - val_accuracy: 0.8169 - 127s/epoch - 163ms/step\n",
      "Epoch 11/15\n",
      "782/782 - 131s - loss: 0.0132 - accuracy: 0.9962 - val_loss: 1.0199 - val_accuracy: 0.8139 - 131s/epoch - 168ms/step\n",
      "Epoch 12/15\n",
      "782/782 - 151s - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.8046 - val_accuracy: 0.8102 - 151s/epoch - 193ms/step\n",
      "Epoch 13/15\n",
      "782/782 - 127s - loss: 0.0094 - accuracy: 0.9971 - val_loss: 1.1437 - val_accuracy: 0.8133 - 127s/epoch - 163ms/step\n",
      "Epoch 14/15\n",
      "782/782 - 133s - loss: 0.0100 - accuracy: 0.9967 - val_loss: 1.1327 - val_accuracy: 0.8023 - 133s/epoch - 170ms/step\n",
      "Epoch 15/15\n",
      "782/782 - 157s - loss: 0.0133 - accuracy: 0.9961 - val_loss: 1.2783 - val_accuracy: 0.8010 - 157s/epoch - 201ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c56aa86d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54be6ec",
   "metadata": {},
   "source": [
    "*Bien, evaluemos la precisión de nuestro modelo:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c95971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 16s - loss: 1.2783 - accuracy: 0.8010 - 16s/epoch - 21ms/step\n",
      "Test score: 1.2783071994781494\n",
      "Test accuracy: 0.8009999990463257\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78956bcd",
   "metadata": {},
   "source": [
    "*80% ¿eh? No está mal, teniendo en cuenta que nos limitamos a solo las primeras 80 palabras de cada revisión.*\n",
    "\n",
    "*Tenga en cuenta que la precisión de validación mientras estábamos entrenando nunca mejoró realmente después de la primera época; Es probable que estemos sobreadaptados. Este es un caso en el que la interrupción temprana habría sido beneficiosa.*\n",
    "\n",
    "*Pero de nuevo, ¡detente y piensa en lo que acabamos de hacer aquí! Una red neuronal que puede \"leer\" reseñas y deducir si al autor le gustó la película o no basándose en ese texto. Y tiene en cuenta el contexto de cada palabra y su posición en la revisión, ¡y configurar el modelo en sí fue solo unas pocas líneas de código! Es bastante increíble lo que puedes hacer con Keras.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7d1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
