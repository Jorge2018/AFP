{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b49313",
   "metadata": {},
   "source": [
    "# Data warehousing\n",
    "\n",
    "*Qué es el almacenamiento de datos. Una base de datos grande y centralizada que contiene información de muchas fuentes. A menudo se utiliza para el análisis de negocios en grandes corporaciones u organizaciones. Querid a través de SQL o herramientas (ej: Tableau). A menudo, departamentos enteros se dedican a mantener un almacén de datos. La normalización de datos es complicada: ¿cómo se relaciona toda esta fecha entre sí? ¿Qué puntos de vista necesita la gente?. Mantener las fuentes de datos es mucho trabajo. Escalar es complicado*\n",
    "\n",
    "\n",
    "# ETL: Extraccion, transformacion y carga\n",
    "\n",
    "*ETL y ELT se refieren a cómo los datos entran en un almacén de datos. Tradicionalmente, el flujo fue extraído, transformado, cargado: los datos sin procesar del sistema operativo se extraen primero periódicamente. Luego, los datos se transforman en el esquema que necesita el DW. Finalmente, los datos se cargan en el almacén de datos, ya en la estructura necesaria. Pero, ¿y si estamos tratando con \"big data\"? Ese paso de transformación puede convertirse en un gran problema*\n",
    "\n",
    "\n",
    "# ELT: Extraccion,carga y transformacion\n",
    "\n",
    "*Hoy en día, una instancia de Oracle Hug no es la única opción para un gran almacén de datos. Piensa que hive le permite alojar bases de datos masivas en un clúster de hadoop.  O bien, puede almacenarlo en un gran almacén de datos NoSQL distribuido... y consultarlo usando cosas como spark o MapReduce. La escalabilidad de Hadoop le permite darle la vuelta al proceso de carga. Extraiga datos sin procesar como antes. Cárguelo tal cual. A continuación, utilice el poder de Hadoop para transformarlo en el lugar*\n",
    "\n",
    "**Mucho más para explorar. El almacenamiento de datos es una disciplina en sí misma, demasiado grande para cubrir aquí. Consulte otros cursos sobre Big-Data, Spark y MapReduce. Cubriremos Spark con más profundidad más adelante en este curso.**\n",
    "\n",
    "# Aprendizaje por refuerzo. \n",
    "\n",
    "*Tienes algún tipo de agente que \"explora\" algún espacio. A medida que avanza, el valor de los diferentes cambios de estado en diferentes condiciones. Esos valores informan el comportamiento posterior del agente. Ejemplos: Pac-man, juego Cat&Mouse. Rendimiento rápido en línea una vez que se han explorado los espacios*\n",
    "\n",
    "# Q-Learning. \n",
    "\n",
    "*Una implementación específica del aprendizaje por refuerzo. Usted tiene: Un conjunto de estados ambientales s . Un conjunto de acciones posibles en esos estados a. Un valor de cada estado/acción Q. Comience con Q valores de 0. Explora el espacio. A medida que suceden cosas malas después de un estado / acción dado, reduzca su Q. A medida que las recompensas ocurren después de un estado / acción determinado, aumente su Q*\n",
    "\n",
    "*¿Cuáles son algunos estados/acciones aquí?. Pac-man tiene un muro al oeste. Pac-man muere si se mueve un paso hacia el sur. Pac-man simplemente continúa viviendo de ir al Nort o al Este. Puede \"mirar hacia adelante\" más de un paso utilizando un factor de descuento al calcular Q (aquí s es el estado anterior, s'es el estado actual). Q(s,a)=descuento * (recompensa(s,a) + max(Q(s'))-Q(s,a))*\n",
    "\n",
    "**El problema de la exploración.**\n",
    "¿Cómo exploramos eficientemente todos los estados posibles? Enfoque simple: elija siempre la acción para un estado dado con la Q más alta. Si hay un empate, elige al azar. Pero eso es realmente ineficiente, y es posible que te pierdas muchos caminos de esa manera. Mejor manera: introducir un término épsilon. Si un número aleatorio es menor que épsilon, no sigas la Q más alta, sino elige al azar. De esa manera, la exploración nunca se detiene totalmente. Elegir épsilon puede ser complicado.\n",
    "\n",
    "**Palabras elegantes.**\n",
    "*Proceso de decisión de Markov. De wikipedia: MDP, proporcionan un marco matemático para modelar la toma de decisiones en situaciones donde los resultados son en parte aleatorios y en parte bajo el control de un tomador de decisiones. ¿Te suena familiar? Los MDP son solo una forma de describir lo que acabamos de hacer usando notación metemática. Los estados todavía se describen como s y s'. Las funciones de transición de estado se describen como Pa(S,S'). Nuestros valores Q se describen como una función de recompensa Ra(S,S'). ¡Incluso palabras más elegantes! ah MDP es un proceso de control estocástico de tiempo discreto*\n",
    "\n",
    "**¡Más palabras elegantes!.**\n",
    "*La programación dinámica, de wikipedia, es un método para resolver un problema complejo dividiéndolo en una colección de subproblemas más simples, resolviendo cada uno de esos subproblemas solo una vez, y almacenando sus soluciones, idealmente, utilizando una estructura basada en la memoria.La próxima vez que ocurran los mismos subproblemas, en lugar de volver a calcular su solución, uno simplemente busca la solución calculada anteriormente, ahorrando así tiempo de cálculo a expensas de un gasto (con suerte) modesto en espacio de almacenamiento.*\n",
    "\n",
    "**Así que para recapitular.**\n",
    "*Puedes hacer un Pac-man inteligente en unos pocos pasos: Pídele que explore semi-aleatoriamente diferentes opciones de movimiento (acciones) dadas diferentes condiciones (estados). Lleve un registro de la recompensa o penalización asociada con cada opción para un estado / acción determinada (Q). use esos valores Q almacenados para informar su elección futura. Concepto bastante simple. Pero bueno, ahora puedes decir que entiendes el aprendizaje por refuerzo, Q-learning, el proceso de decisión de Markov y la programación dinámica.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485bf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
