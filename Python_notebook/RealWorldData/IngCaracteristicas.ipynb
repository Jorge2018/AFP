{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0971fdb1",
   "metadata": {},
   "source": [
    "# ¿Qué es la ingeniería de características?. \n",
    "\n",
    "*Aplicar su conocimiento de los datos, y el modelo que está utilizando, para crear mejores características para entrenar su modelo. ¿Qué funciones debo usar? ¿Necesito transformar estas características de alguna manera? ¿Cómo manejo los datos que faltan? ¿Debo crear nuevas características a partir de las existentes? No puedes simplemente agregar datos sin procesar y esperar buenos resultados. Este es el arte del aprendizaje automático; donde se aplica la experiencia. \"El aprendizaje automático aplicado es básicamente ingeniería de características\" Andrew Ng*\n",
    "\n",
    "**La maldición de la dimensionalidad.**\n",
    "\n",
    "*Demasiadas características pueden ser un problema, conduce a datos dispersos. Cada característica es una nueva dimensión. Gran parte de la ingeniería de características es seleccionar las características más relevantes para el problema en cuestión. Aquí es a menudo donde entra en juego el conocimiento del dominio. Las técnicas de reducción de dimensionalidad no supervisadas también se pueden emplear para destilar muchas características en menos características. PCA, K-Means*\n",
    "\n",
    "**Imputación de datos faltantes: Reemplazo medio.**\n",
    "\n",
    "*Reemplace los valores faltantes con el valor medio del resto de la columna (¡columnas, no filas! una columna representa una sola característica; solo tiene sentido tomar la media de otras muestras de la misma característica). Rápido y fácil, no afectará la media o el tamaño de la muestra del conjunto de datos general. La mediana puede ser una mejor opción que la media cuando hay valores atípicos. Pero en general es bastante terrible. Solo funciona a nivel de columna, pierde correlaciones entre entidades.No se puede usar en características categóricas (sin embargo, imputar con el valor más frecuente puede funcionar en este caso). No es muy preciso*\n",
    "*si no hay muchas filas que contengan datos faltantes. Y eliminar esas filas no sesga sus datos. Y no tienes mucho tiempo. Tal vez sea algo razonable. Pero nunca va a ser la respuesta correcta para el \"mejor\" enfoque. Casi cualquier cosa es mejor. ¿Puedes sustituirlo por un campo similar más similar? (es decir, resumen de revisión vs. texto completo)*\n",
    "\n",
    "**Imputación de datos faltantes: Aprendizaje automatico**\n",
    "\n",
    "**KNN: encuentra k filas \"más cercanas\" (más similares) y promedia sus valores. Asume datos numéricos, no categóricos. Hay formas de manejar los datos categóricos (distancia de hamming), pero los datos categóricos probablemente estén mejor atendidos.**\n",
    "\n",
    "**Aprendizaje profundo (Deep Learning): ¡cree un modelo de aprendizaje automático para imputar datos para su modelo de aprendizaje automático! Funciona bien para datos categóricos. Muy bien. Pero es complicado.** \n",
    "\n",
    "**Regresión: Buscar relaciones lineales o no lineales entre la entidad que falta y otras características, técnica más avanzada: MICE (Imputación múltiple por ecuaciones encadenadas)**\n",
    "\n",
    "**Imputación de datos faltantes: Solo obten mas datos.**\n",
    "\n",
    "*¿Qué es mejor que imputar datos? ¡Obteniendo más datos reales!. A veces solo tienes que esforzarte más o recopilar más datos*\n",
    "\n",
    "**¿Qué son los datos desequilibrados?**\n",
    "*Gran discrepancia entre los casos \"positivos\" y \"negativos\". es decir, detección de fraude. El fraude es raro, y la mayoría de las filas no serán fraude. No dejes que la terminología te confunda; \"Positivo\" no significa \"bueno\". Significa que lo que estás probando es lo que sucedió. Si su modelo de aprendizaje automático está hecho para detectar fraude, entonces el fraude es el caso positivo. Principalmente un problema con las redes neuronales*\n",
    "\n",
    "**Sobremuestreo. Muestras duplicadas de la clase minoritaria. Se puede hacer al azar**\n",
    "\n",
    "**En lugar de crear más positivos, elimine los negativos. Tirar datos a la basura generalmente no es la respuesta correcta. A menos que esté tratando específicamente de evitar problemas de escalado de \"big data\"**\n",
    "\n",
    "**SMOTE**\n",
    "*Técnica de sobremuestreo de minorías sintéticas. Generar artificialmente nuevas muestras de la clase minoritaria utilizando vecinos más cercanos. Ejecute K-vecinos más cercanos de cada muestra de la clase minoritaria. Cree una nueva muestra a partir del resultado KNN (media de los vecinos). Tanto genera nuevas muestras como submuestras de clase mayoritaria. En general, es mejor que simplemente sobremuestrear*\n",
    "\n",
    "**Ajuste de umbrales.(Adjusting Threshold)**\n",
    "\n",
    "*Al hacer predicciones sobre una clasificación (fraude / no fraude), tiene algún tipo de umbral de probabilidad en el que marcará algo como el caso positivo (fraude). Si tiene demasiados falsos positivos, una forma de solucionarlo es simplemente aumentar ese umbral. Garantía para reducir los falsos positivos. Pero podría resultar en más falsos negativos.*\n",
    "\n",
    "**Binning**\n",
    "\n",
    "*Observaciones de cubo basadas en rangos de valores. Ejemplo: edades estimadas de las personas. Pon a los 20 y tantos en una clasificación, a los 30 y tantos en otra, etc. El binning cuantil clasifica los datos por su lugar en la distribución de datos. Asegure un tamaño uniforme de los contenedores. Transformar datos numéricos en datos ordinales. Especialmente útil cuando hay incertidumbre en las mediciones*\n",
    "\n",
    "**Transforming**\n",
    "\n",
    "*Datos de características con una tendencia exponencial mi beneficio de una transformación logarítmica. Aplicar alguna función a una característica para que sea más adecuada para el entrenamiento. Ejemplo: recomendaciones de Youtube. Una característica numérica x también se representa por (x*x) y Vx. Esto permite el aprendizaje de funciones superlineales y sublineales.*\n",
    "\n",
    "**Encoding**\n",
    "\n",
    "*Transformar los datos en una nueva representación requerida por el modelo. Codificación en caliente. Crea \"buckets\" para cada categoría. El cubo de su categoría tiene un 1, todos los demás tienen un 0. Muy común en el aprendizaje profundo, donde las categorías están representadas por las neuronas de salida individuales*\n",
    "\n",
    "**Scaling/Normalization**\n",
    "\n",
    "*Algunos modelos prefieren que los datos de características se distribuyan normalmente alrededor de 0 (la mayoría de las redes neuronales). La mayoría de los modelos requieren que los datos de características se escalen al menos a valores comparables. De lo contrario, las características con magnitudes más grandes tendrán más peso del que deberían. Ejemplo: modelar la edad y los ingresos como características-ingresos serán valores mucho más altos que las edades. Scikit_learn tiene un módulo de preprocesador que ayuda (MinMaxScaler, etc.).  Recuerde escalar su copia de seguridad de resultados*\n",
    "\n",
    "**Shuffling**\n",
    "\n",
    "*Muchos algoritmos se benefician de barajar sus datos de entrenamiento. De lo contrario, pueden aprender de las señales residuales en los datos de entrenamiento resultantes del orden en que se recopilaron.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7b6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
