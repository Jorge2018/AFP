{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80405a9",
   "metadata": {},
   "source": [
    "# Limpieza de datos\n",
    "\n",
    "*La realidad es que gran parte de su tiempo como científico de datos se dedicará a preparar y limpiar sus datos. Valores atípicos, datos faltantes, datos maliciosos, datos erróneos, datos irrelevantes, datos incoherentes, formato*\n",
    "*Basura dentro, basura fuera. ¡Mira tus datos! ¡Examínalo!. Y siempre haz esto, no solo cuando no obtengas un resultado que te guste.*\n",
    "\n",
    "**Analicemos algunos datos de registro web. Todo lo que quiero son las páginas más populares en mi sitio web de noticias sin fines de lucro. ¿Qué tan difícil puede ser eso?**\n",
    "\n",
    "*¡Tomemos un registro de acceso web y descubramos las páginas más vistas en un sitio web desde él! Suena fácil, ¿verdad?\n",
    "Configuremos un regex que nos permita analizar una línea de registro de acceso de Apache:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300fabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "format_pat= re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "logPath = r\"C:\\Users\\Dell Inspiron\\Python_notebook\\csv\\DataScience-Python3\\access_log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9952",
   "metadata": {},
   "source": [
    "*Ahora vamos a crear un pequeño script para extraer la URL en cada acceso, y vamos a utilizar un diccionario para contar el número de veces que aparece cada uno. Luego lo ordenaremos e imprimiremos las 20 páginas principales. ¿Qué podría salir mal?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1c1d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/: 77\n",
      "/orlando-headlines/: 36\n",
      "/comics-2/: 12\n",
      "/world/: 12\n",
      "/weather/: 4\n",
      "/australia/: 4\n",
      "/about/: 4\n",
      "/national-headlines/: 3\n",
      "/feed/: 2\n",
      "/sample-page/feed/: 2\n",
      "/science/: 2\n",
      "/technology/: 2\n",
      "/entertainment/: 1\n",
      "/san-jose-headlines/: 1\n",
      "/business/: 1\n",
      "/travel/feed/: 1\n"
     ]
    }
   ],
   "source": [
    "URLCounts = {}\n",
    "\n",
    "with open(logPath, \"r\") as f:\n",
    "    for line in (l.rstrip() for l in f):\n",
    "        match= format_pat.match(line)\n",
    "        if match:\n",
    "            access = match.groupdict()\n",
    "            agent = access['user_agent']\n",
    "            if (not('bot' in agent or 'spider' in agent or \n",
    "                    'Bot' in agent or 'Spider' in agent or\n",
    "                    'W3 Total Cache' in agent or agent =='-')):\n",
    "                request = access['request']\n",
    "                fields = request.split()\n",
    "                if (len(fields) == 3):\n",
    "                    (action, URL, protocol) = fields\n",
    "                    if (URL.endswith(\"/\")):\n",
    "                        if (action == 'GET'):\n",
    "                            if URL in URLCounts:\n",
    "                                URLCounts[URL] = URLCounts[URL] + 1\n",
    "                            else:\n",
    "                                URLCounts[URL] = 1\n",
    "\n",
    "results = sorted(URLCounts, key=lambda i: int(URLCounts[i]), reverse=True)\n",
    "\n",
    "for result in results[:20]:\n",
    "    print(result + \": \" + str(URLCounts[result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de14c1",
   "metadata": {},
   "source": [
    "*¡Esto está empezando a parecer más creíble! Pero si profundizaras aún más, encontrarías que las páginas /feed/ son sospechosas, y algunos robots todavía se están deslizando. Sin embargo, es correcto decir que las noticias de Orlando, las noticias mundiales y los cómics son las páginas más populares a las que accede un humano real en este día.*\n",
    "\n",
    "*La moraleja de la historia es: ¡conozca sus datos! Y siempre cuestione y examine sus resultados antes de tomar decisiones basadas en ellos. Si su empresa toma una mala decisión porque proporcionó un análisis de datos de origen incorrectos, podría meterse en problemas reales. Asegúrese de que las decisiones que tome al limpiar sus datos también sean justificables: ¡no elimine los datos solo porque no respaldan los resultados que desea!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440617a",
   "metadata": {},
   "source": [
    "# La importancia de la normalización de datos. \n",
    "\n",
    "*Si su modelo se basa en varios atributos numéricos, ¿son comparables?. Ejemplo: las edades pueden variar de 0 a 100 y los ingresos de 0 mil millones. Algunos modelos pueden no funcionar bien cuando diferentes atributos están en escalas muy diferentes. Puede hacer que algunos atributos cuenten más que otros. El sesgo en los atributos también puede ser un problema*\n",
    "\n",
    "**Ejemplos**\n",
    "\n",
    "*La implementación de PCA de scikit-learn tiene una opción de \"blanquear\" que hace esto por usted. Úsalo. Scikit-Learn tiene un módulo de preprocesamiento con prácticas funciones de normalización y escala. Sus datos pueden tener \"sí\" y \"no\" que deben convertirse a \"1\" o \"0\"*\n",
    "*Lea los documentos. La mayoría de las técnicas de minería de datos y aprendizaje automático funcionan bien con datos sin procesar y no normalizados. Pero verifique el que está usando antes de comenzar. ¡No olvides volver a escalar tus resultados cuando hayas terminado!*\n",
    "\n",
    "**Tratar con valores atípicos.**\n",
    "\n",
    "*A veces es apropiado eliminar valores atípicos de sus datos de entrenamiento. ¡Haz esto responsablemente! Entiende por qué estás haciendo esto. Por ejemplo: en el filtrado colaborativo, un solo usuario que califique miles de películas podría tener un gran efecto en las calificaciones de todos los demás. Eso puede no ser deseable. Otro ejemplo: en los datos de registro wen, los valores atípicos pueden representar bots u otros agentes que deben descartarse. Pero si alguien realmente quiere el ingreso medio de los ciudadanos estadounidenses por ejemplo.No echas a los multimillonarios solo porque quieres*\n",
    "\n",
    "*Nuestro viejo amigo Standart Desviation proporcionó una forma basada en principios para clasificar los valores atípicos. Encuentra puntos de datos más que algún múltiplo de una desviación estándar en sus datos de entrenamiento. ¿Qué múltiple? Solo tienes que usar el sentido común*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
