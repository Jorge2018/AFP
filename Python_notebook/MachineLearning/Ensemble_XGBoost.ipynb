{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22a1a79",
   "metadata": {},
   "source": [
    "# Aprendizaje conjunto\n",
    "\n",
    "*Random Forest fue un ejemplo de aprendizaje en conjunto. Simplemente significa que usamos múltiples modelos para tratar de resolver el mismo problema, y dejar que elVotación de los resultados*\n",
    "\n",
    "*Random Forest utiliza el bagging (bootstrap aggregating) para implementar el aprendizaje por conjunto. Muchos modelos se construyen entrenando en subconjuntos de datos dibujados aleatoriamente. Boosting i una técnica alternativa donde cada modelo posterior en el conjunto arranca atributos que abordan datos mal clasificados por el modelo anterior. Un grupo de modelos entrena varios modelos diferentes utilizando datos de entrenamiento. y elige el que mejor funciona con los datos de prueba. El apilamiento ejecuta varios modelos a la vez en los datos y combina los resultados. ¡Así es como se ganó el premio de Netflix!*\n",
    "\n",
    "**Aprendizaje avanzado en conjunto: formas de hacerlo inteligente**\n",
    "\n",
    "*Clasificador óptimo Bayes. teóricamente el mejor, pero casi siempre poco práctico. Promedio de parámetros bayesianos. Intenta hacer que BOC sea práctico, pero aún se malinterpreta, es susceptible al sobreajuste y, a menudo, superado por el enfoque de embolsado más simple. Combinación de modelos bayesianos. Trata de abordar todos esos problemas. Pero al final, es casi lo mismo que usar la validación cruzada para encontrar las mejores combinaciones de modelos.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75b627",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "*Árboles potenciados por gradiente extremo. Recuerde que impulsar es un método de conjunto. Cada árbol aumenta los atributos que llevaron a clasificaciones erróneas del árbol anterior. Es increíble. Rutinariamente gana competiciones de kaggle. Fácil de usar. Rápido. Una buena opción para un algorithm para empezar*\n",
    "\n",
    "**Caracterirsticas de XGBoost**\n",
    "\n",
    "*Refuerzo regularizado (evitar el sobreajuste). Puede manejar los valores faltantes automáticamente. Procesamiento paralelo. Puede realizar una validación cruzada en cada iteración. Permite la detención temprana, encontrando el número óptimo de iteraciones. Entrenamiento incremental. Puede conectar sus propios objetivos de optimización. Poda de árboles. Generalmente resulta en árboles más profundos, pero optimizados,*\n",
    "\n",
    "**Usos de XGBoost**\n",
    "\n",
    "*No solo está hecho para scikit-learn, por lo que tiene su propia interfaz. Utiliza la estructura DMatrix para contener características y etiquetas. Sin embargo, puede crear esto fácilmente a partir de una matriz numpy. Todos los parámetros se transmiten a través de un diccionario. Llama al train, luego predice. Es fácil*\n",
    "\n",
    "**XGBoost hiperparametros**\n",
    "\n",
    "*booster. gbtree o gblinear. Objetivo. (es decir, multi:softmax, multi:softprob). Eta. (La tasa de aprendizaje ajusta los pesos en cada paso). Max_depth (profundidad del árbol). Min_child_weight. Puede controlar el sobreajuste, pero demasiado alto se ajustará poco, y muchos otros*\n",
    "\n",
    "**Es casi todo lo que necesita saber para ML en términos prácticos, en la lista para problemas simples de clasificación o regresión. Veámoslo en acción**\n",
    "\n",
    "*Experimentemos usando el conjunto de datos del iris. Este conjunto de datos incluye el ancho y la longitud de los pétalos y sépalos de muchas flores de iris, y la especie específica de iris a la que pertenece la flor. Nuestro desafío es predecir las especies de una muestra de flores solo basándonos en los tamaños de sus pétalos. Revisaremos este conjunto de datos más adelante cuando hablemos también sobre el análisis de componentes principales.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff21367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "4\n",
      "['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "\n",
    "numSamples,numFeatures=iris.data.shape\n",
    "\n",
    "print(numSamples)\n",
    "print(numFeatures)\n",
    "print(list(iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b909823",
   "metadata": {},
   "source": [
    "*Dividamos nuestro conjunto de datos en un 20% reservado para el modelo de prueba y el 80% restante para entrenarlo. Al retener nuestros datos de prueba, podemos asegurarnos de que estamos evaluando los resultados en función de las nuevas flores que no ha visto antes. Por lo general, nos referimos a nuestras características (en este caso, los tamaños de pétalos) como X, y las etiquetas (en este caso, la especie) como y.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64784cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(iris.data,iris.target,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88eec3",
   "metadata": {},
   "source": [
    "*ahora cargaremos XGBoost y convertiremos nuestros datos al formato DMatrix que espera. Uno para los datos de entrenamiento y otro para los datos de prueba*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb4f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "train=xgb.DMatrix(X_train, label=y_train)\n",
    "test=xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05de6c9",
   "metadata": {},
   "source": [
    "*Ahora definiremos nuestros hiperparámetros. Estamos eligiendo softmax ya que este es un problema de clasificación múltiple, pero los otros parámetros idealmente deberían ajustarse a través de la experimentación.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf96d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "param={\n",
    "    'max_depth':4,\n",
    "    'eta':0.3,\n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':3}\n",
    "\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043c2e4",
   "metadata": {},
   "source": [
    "*Sigamos adelante y entrenemos nuestro modelo usando estos parámetros como una primera suposición*\n",
    "*Luego usaremos el modelo entrenado para predecir las clasificaciones de los datos que reservamos para las pruebas. Cada número de clasificación que obtenemos corresponde a una especie específica de iris.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8ded3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 2. 0. 2. 0. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 1. 0. 0. 2. 1. 0. 0.\n",
      " 2. 0. 0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "model=xgb.train(param,train,epochs)\n",
    "predictions=model.predict(test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b7482",
   "metadata": {},
   "source": [
    "*Midamos la precisión de los datos de prueba*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b188ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd8cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
