{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481d64d3",
   "metadata": {},
   "source": [
    "**Redes generativas antagónicas. (GAN)**\n",
    "\n",
    "*Sí, es la tecnología detrás de los \"deepfakes\" y todas esas aplicaciones virales de intercambio de caras y envejecimiento. Pero los investigadores tenían intenciones más nobles. Generar conjuntos de datos sintéticos para eliminar información privada.  Detección de anomalías. Conducción autónoma. Arte, música*\n",
    "\n",
    "*Aprende la distribución real de vectores latentes. No asume distribuciones normales gaussianas como las VAE. El generador mapea el ruido aleatorio (!) a una distribución de probabilidad. El discriminador aprende a identificar imágenes reales a partir de imágenes generadas (falsas). El generador está tratando de engañar al discriminador para que piense que sus imágenes son reales. El discriminador está tratando de atrapar el generador. El generador y el discriminador son adversarios, de ahí el nombre. Una vez que el discriminador ya no puede notar la diferencia, hemos terminado (en teoría)*\n",
    "\n",
    "**Matemáticas elegantes.**\n",
    "\n",
    "*Esa es la función de pérdida adversarial. Lo llamamos un \"juego min-max\". El generador está minimizando su pérdida en la creación de imágenes realistas. El discriminador, al mismo tiempo, está maximizando su capacidad para detectar falsificaciones. Es complicado y delicado. El entrenamiento es muy inestable; Un montón de prueba y error / ajuste de hiperparámetros. Colapso del modo. Degradados que se desvanecen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5718e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# maintain consistent performance\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# confirm GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c322fad",
   "metadata": {},
   "source": [
    "*Como antes, cargaremos el conjunto de datos de Fashion MNIST, lo fusionaremos todo ya que no estamos haciendo entrenamiento / prueba, y normalizaremos los datos como lo hicimos con VAE:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccc0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebed5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# merge the training and testing sets\n",
    "dataset = np.concatenate([x_train, x_test], axis=0)\n",
    "# normalize the images from [0,255] to [0,1]\n",
    "dataset = np.expand_dims(dataset, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636e5d0",
   "metadata": {},
   "source": [
    "*Ahora remodelaremos los datos a las dimensiones necesarias para las capas de CNN, los barajaremos y los agruparemos por lotes:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94de1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# convolution layers work 3 channels\n",
    "dataset = np.reshape(dataset, (-1, 28, 28, 1))\n",
    "# create a tensorflow dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "# set the batch size otherwise it reads one image at a time\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88768022",
   "metadata": {},
   "source": [
    "*Ahora configuremos el modelo para nuestro generador. Tenga en cuenta el hiperparámetro NOISE_DIM. A partir de ahí tenemos tres capas Conv2DTranspose para trabajar nuestro camino hacia una imagen final generada.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44246ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1894144   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 256)      590080    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 128)      295040    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1153      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,780,417\n",
      "Trainable params: 2,780,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# the generator's input is a noise vector\n",
    "# hyper-parameter that also requires fine-tuning\n",
    "NOISE_DIM = 150\n",
    "\n",
    "# design a generator model with upsampling layers\n",
    "# in GANs practices, usually the generator has leaky relu activation while the discriminator has relu\n",
    "generator = keras.models.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(NOISE_DIM,)),\n",
    "  layers.Dense(7*7*256),\n",
    "  layers.Reshape(target_shape=(7, 7, 256)),\n",
    "  layers.Conv2DTranspose(256, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\"),\n",
    "  layers.Conv2DTranspose(128, 3, activation=\"LeakyReLU\", strides=2, padding=\"same\"),\n",
    "  layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
    "])\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d970dc",
   "metadata": {},
   "source": [
    "*A continuación, haremos nuestro modelo discriminador, utilizando dos capas Conv2D para reducir el muestreo antes de entrar en una capa densa de 64 neuronas y una caída para evitar el sobreajuste. Su salida es binaria, ya que su trabajo es clasificar las imágenes como \"reales\" o \"falsas\"... hasta que ya no puede notar la diferencia.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf8a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 256)       2560      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 128)         295040    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                401472    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699,137\n",
      "Trainable params: 699,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.models.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "  layers.Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "  layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67aa6c3",
   "metadata": {},
   "source": [
    "*Configure nuestros optimizadores, función de pérdida y métricas de precisión. Vaya, más hiperparámetros: la clave es obtener las tasas de aprendizaje correctas tanto en el generador como en el discriminador. De lo contrario, no será estable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40f41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "optimizerD = keras.optimizers.Adam(learning_rate=0.00003, beta_1=0.5)\n",
    "\n",
    "# binary classifier (real or fake)\n",
    "lossFn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# accuracy metric\n",
    "gAccMetric = tf.keras.metrics.BinaryAccuracy()\n",
    "dAccMetric = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a47d4",
   "metadata": {},
   "source": [
    "*Bien, vamos a unirlos. Inicialmente alimentamos nuestro generador con ruido gaussiano. Sus resultados \"falsos\" se concatenan a datos \"reales\" introducidos en el discriminador. El discriminador hace todo lo posible para identificar etiquetas reales vs. falsas. Aquí definimos la función de entrenamiento para el discriminador:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b020afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def trainDStep(data):\n",
    "  # the batch is (32,28,28,1), so extract 32 value\n",
    "  batchSize = tf.shape(data)[0]\n",
    "  # create a noise vector as generator input sampled from Gaussian Random Normal\n",
    "  # As an exercise try sampling from a uniform distribution and observe the difference\n",
    "  noise = tf.random.normal(shape=(batchSize, NOISE_DIM))\n",
    "\n",
    "  # concatenate the real and fake labels\n",
    "  y_true = tf.concat(\n",
    "    [\n",
    "      # the original data is real, labeled with 1\n",
    "      tf.ones(batchSize, 1),\n",
    "      # the forged data is fake, labeled with 0\n",
    "      tf.zeros(batchSize, 1)\n",
    "    ],\n",
    "    axis=0\n",
    "  )\n",
    "\n",
    "  # record the calculated gradients\n",
    "  with tf.GradientTape() as tape:\n",
    "    # generate forged samples\n",
    "    fake = generator(noise)\n",
    "    # concatenate real data and forged data\n",
    "    x = tf.concat([data, fake], axis=0)\n",
    "    # see if the discriminator detects them\n",
    "    y_pred = discriminator(x)\n",
    "    # calculate the loss\n",
    "    discriminatorLoss = lossFn(y_true, y_pred)\n",
    "\n",
    "  # apply the backward path and update weights\n",
    "  grads = tape.gradient(discriminatorLoss, discriminator.trainable_weights)\n",
    "  optimizerD.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "  # report accuracy\n",
    "  dAccMetric.update_state(y_true, y_pred)\n",
    "\n",
    "  # return the loss for visualization\n",
    "  return {\n",
    "      \"discriminator_loss\": discriminatorLoss,\n",
    "      \"discriminator_accuracy\": dAccMetric.result()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca156ca",
   "metadata": {},
   "source": [
    "*Y ahora, la función de entrenamiento para el generador. Recuerde que el generador está tratando de no ser atrapado, por lo que quiere ser clasificado como real. Su función de pérdida se basa en la frecuencia con la que el discriminador clasifica sus muestras falsas como reales. Esto está en tensión con el discriminador, que está tratando de encontrar falsificaciones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580286b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def trainGStep(data):\n",
    "  batchSize = tf.shape(data)[0]\n",
    "  noise = tf.random.normal(shape=(batchSize, NOISE_DIM))\n",
    "  # when training the generator, we want it to maximize the probability that its\n",
    "  # output is classified as real, remember the min-max game\n",
    "  y_true = tf.ones(batchSize, 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_pred = discriminator(generator(noise))\n",
    "    generatorLoss = lossFn(y_true, y_pred)\n",
    "\n",
    "  grads = tape.gradient(generatorLoss, generator.trainable_weights)\n",
    "  optimizerG.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "  gAccMetric.update_state(y_true, y_pred)\n",
    "\n",
    "  return {\n",
    "      \"generator_loss\": generatorLoss,\n",
    "      \"generator_accuracy\": gAccMetric.result()\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de83f7",
   "metadata": {},
   "source": [
    "*Configuremos una práctica función para visualizar las imágenes generadas mientras entrenamos:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8261ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plotImages(model):\n",
    "    images = model(np.random.normal(size=(81, NOISE_DIM)))\n",
    "\n",
    "    plt.figure(figsize=(9, 9))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(9,9,i+1)\n",
    "        plt.imshow(np.squeeze(image, -1), cmap=\"Greys_r\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21361b",
   "metadata": {},
   "source": [
    "*Ahora es la hora del espectáculo. Dado que el juego de confrontación es un equilibrio delicado, queremos observar los resultados a medida que avanza el entrenamiento. Puede converger y luego divergir de nuevo; Es algo muy delicado. Aquí simplemente revisamos cada lote, entrenamos al discriminador y entrenamos al generador en cada época. Cada dos épocas echaremos un vistazo a algunas imágenes generadas para ver cómo se ven, ya que el generador aprende distribuciones de probabilidad para representar imágenes realistas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5a7842",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_216\\870233026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mgLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainGStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mgLossSum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgLoss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'generator_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mgAccSum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgLoss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'generator_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "\n",
    "  # accumulate the loss to calculate the average at the end of the epoch\n",
    "  dLossSum = 0\n",
    "  gLossSum = 0\n",
    "  dAccSum = 0\n",
    "  gAccSum = 0\n",
    "  cnt = 0\n",
    "\n",
    "  # loop the dataset one batch at a time\n",
    "  for batch in dataset:\n",
    "\n",
    "    # train the discriminator\n",
    "    # remember you could repeat these 2 lines of code for K times\n",
    "    dLoss = trainDStep(batch)\n",
    "    dLossSum += dLoss['discriminator_loss']\n",
    "    dAccSum += dLoss['discriminator_accuracy']\n",
    "\n",
    "    # train the generator\n",
    "    gLoss = trainGStep(batch)\n",
    "    gLossSum += gLoss['generator_loss']\n",
    "    gAccSum += gLoss['generator_accuracy']\n",
    "\n",
    "    # increment the counter\n",
    "    cnt += 1\n",
    "\n",
    "  # log the performance\n",
    "  print(\"E:{}, Loss G:{:0.4f}, Loss D:{:0.4f}, Acc G:%{:0.2f}, Acc D:%{:0.2f}\".format(\n",
    "      epoch,\n",
    "      gLossSum/cnt,\n",
    "      dLossSum/cnt,\n",
    "      100 * gAccSum/cnt,\n",
    "      100 * dAccSum/cnt\n",
    "  ))\n",
    "    \n",
    "  if epoch % 2 == 0:\n",
    "    plotImages(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = generator(np.random.normal(size=(81, NOISE_DIM)))\n",
    "\n",
    "# plot the generated samples\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(9,9,i+1)\n",
    "    plt.imshow(np.squeeze(image, -1), cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676100c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
